---
title: "Apprentissage supervisé - TP1"
output:
  html_document:
    code_folding: show
    toc: true
    toc_depth: 2
    theme: cerulean
    highlight: tango
    css: style.css
---

```{r initial_chunk, echo = FALSE, warning = FALSE, message = FALSE}
library("knitr")
opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE, cache = FALSE, fig.align = 'center', dpi = 300, out.width = '75%')
```

Les différentes librairies qui seront utilisés pour ce TP sont listés ici. Le code pour générer ce document ainsi que le code R qui a servi de base peuvent être trouvé [ici](https://github.com/MathieuMarauri/apprentissage-2020/blob/master/tp1/output/correction.Rmd) et [là](https://github.com/MathieuMarauri/apprentissage-2020/blob/master/tp1/src/TP1.R).

```{r librairies}
library("caret") # ensemble de meta-fonctions nécessaires au processus d'apprentissage supervisé
library("class") # fonction knn
library("e1071") # librairie nécessaire pour la fonction tune.knn
library("FNN") # fast KNN
library("ggplot2") # data visualisation
library("klaR") # fonction Naivebayes utilisée avec la librairie caret
library("mlbench") # dataset Vehicle
library("skimr")
```

```{r ggplot-theme, echo = FALSE}
library("kableExtra") # table formating
library("magrittr") # Pipe operators
# Set default ggplot theme
theme_set(
  theme_light(
  base_size = 15
  ) +
  theme(
    text = element_text(family = "Gibson", colour = "gray10"),
    panel.border = element_blank(),
    axis.line = element_line(colour = "gray50", size = .5),
    axis.ticks = element_blank(),
    strip.background = element_rect(colour = "gray50", fill = "transparent", size = .7),
    strip.text.x = element_text(colour = "gray10"),
    strip.text.y = element_text(colour = "gray10"),
    legend.key.size = unit(1.5, "cm")
  )
)

# Set default scales
scale_colour_continuous <- function(...) ggplot2::scale_colour_viridis_c(..., option = "viridis")
scale_colour_discrete <- function(...) ggplot2::scale_colour_viridis_d(..., option = "viridis")
scale_fill_continuous <- function(...) ggplot2::scale_fill_viridis_c(..., option = "viridis")
scale_fill_discrete <- function(...) ggplot2::scale_fill_viridis_d(..., option = "viridis")
```


# Exercice 1 - Règles et risque de Bayes en discrimination binaire

TODO : correction première question ; explication estimation densité ; explication validation / apprentissage / test ; mlr3 ; naive bayes sans caret ; grid plus courte pour NB

Voici 3 problèmes de discrimination binaire : dans chaque cas on a simulé un échantillon de taille $n = 1000$.

_Cas 1_ 

Pour $i = 1..1000$, $X_i  \mathcal{N}(0, 1)$, $U_i \mathcal{U}[0, 1]$ on a :

$$
Y_i = \begin{cases}
      \mathbb{1}_{U_i \leq 0,1} & \text{si $X_i \leq 0$}\\
      \mathbb{1}_{U_i > 0,2} & \text{si $X_i > 0$}\\
    \end{cases}
$$

_Cas 2_ 

Pour $i = 1..1000$, $X_i  \mathcal{N}(0, 1)$, $U_i \mathcal{U}[0, 1]$ on a :

$$
Y_i = \begin{cases}
      \mathbb{1}_{U_i \leq 0,2} & \text{si $X_i \leq 0$}\\
      \mathbb{1}_{U_i > 0,4} & \text{si $X_i > 0$}\\
    \end{cases}
$$

_Cas 3_

On travaille sur des données réelles issues d'une enquête, à partir d'un échantillon tiré au hasard de $n=1000$ consommateurs de café. La variable à expliquer $Y$ est qualitative binaire et prend les modalités "sucré" et "non sucré". La variable explicative à notre disposition est $X$ qui représente le sexe. On dispose de la table avec en lignes les 1000 consommateurs et en colonnes les 2 variables $X$ et $Y$. Des résultats de statistique bivariée nous donnent :

* Parmi les femmes, on a 20% qui prennent du sucre dans leur café $Y="sucré"$
* Parmi les hommes, on a 10% qui prennent du sucre dans leur café $Y="sucré"$

## Question 1 - Règle de Bayes

>_Donner dans chacun des trois cas, si c'est possible, la règle de Bayes et le risque de Bayes._

## Question 2 - Complexité

>_Est-il possible de donner un indicateur de la complexité de ces problèmes et ainsi de les ordonner en fonction de leur complexité ?_

Le risque de Bayes donne une idée de la complexité du problème. Plus l'erreur théorique minimale est importante plus on considère le problème comme complexe. Ainsi le cas 2 est plus complexe que le premier. 

## Question 3 - Simulation

>_Simuler les données du cas 2 et créer en plus un échantillon test de taille 200 (avec le même modèle)._

On va générer un vecteur de $X$ et un vecteur de $Y$ de longueur 1000. On fixe une graine pour que nos résultats soient reproductibles.

```{r simul_data}
# Nombre d'éléments dans l'échantillon
n_train <- 1000

# Générer X selon une loi normale
set.seed(1)
X <- rnorm(n_train)

# Générer U selon une loi uniforme
set.seed(2)
U <- runif(n_train)

# Générer le vecteur réponse Y selon le cas 2
Y1 <- rep(0, n_train)
Y1[X <= 0 & U <= 0.2] <- 1
Y1[X > 0 & U > 0.4] <- 1

# Y est transformé en factor pour faciliter l'utilisation de la fonction tune.knn dans la suite
Y <- as.factor(Y1)

# Créer un data.frame avec X et Y
donnees <- data.frame(X, Y)
```

Sur le graphe suivant on peut voir la densité de $X$ selon la valeur prise par $Y$.

```{r simul_plot}
ggplot(
  data = donnees,
  mapping = aes(x = X, fill = Y)
) +
  geom_density(
    alpha = .7
  ) +
  labs(
    x = "X",
    y = "Densité",
    title = "Données d'apprentissage",
    fill = "Y"
  )
```

De la même manière on peut simuler des données de test qui seront utilisées pour mesurer l'erreur de nos modèles de prédictions.

```{r simul_data_test}
n_test <- 200
set.seed(3)
X <- rnorm(n_test)
set.seed(4)
U <- runif(n_test)
Y2 <- rep(0, n_test)
Y2[X <= 0 & U <= 0.2] <- 1
Y2[X > 0 & U > 0.4] <- 1
Y <- as.factor(Y2)
test <- data.frame(X, Y)
ggplot(
  data = test,
  mapping = aes(x = X, fill = Y)
) +
  geom_density(
    alpha = .7
  ) +
  labs(
    x = "X",
    y = "Densité",
    title = "Données de test",
    fill = "Y"
  )
```

## Question 4 - Knn

>_Quelle est la règle de décision associée à l'algorithme des k plus proches voisins ? Mettre en oeuvre les k plus proches voisins. Justifier le choix des paramètres et commenter les résultats en validation et sur l'échantillon test._

#### La règle de décision

Dans le cas d'une classification, la règle de décision associée aux $k$ plus-proches-voisins est le vote à la majorité. Pour $k=5$ par exemple, si 3 voisins ont $Y=0$ et 2 voisins ont $Y=1$ alors la prédiction sera $\widehat{Y}=0$.

#### Un premier modèle

On commence par lancer un modèle de k plus-proches-voisins avec un choix arbitraire pour $k$, 10 par exemple. Pour des questions de reproductibilité on fixe là aussi une seed. On utilise la fonction `class::knn()` qui prend 4 paramètres en entrée : 

* `train` : les prédicteurs des données d'apprentissage,
* `test` : les prédicteurs des données de tests sur lesquelles se feront les prédictions,
* `cl` : le vecteur de la variable cible des données d'apprentissage,
* `k` : le nombre de plus-proches-voisins souhaité.

```{r knn}
set.seed(123456)
knn_pred <- knn(
  train = as.matrix(donnees$X), # données d'apprentissage
  test = as.matrix(test$X), # données à prédire
  cl = donnees$Y, # vraies valeurs
  k = 10 # nombre de voisins
)
```

On peut calculer l'erreur et la précision de ce modèle. 

```{r knn_perf, eval = FALSE}
# Taux de bonnes prédictions
sum(knn_pred == test$Y) / n_test

# Taux d'erreur
1 - sum(knn_pred == test$Y) / n_test
```

L'erreur est de `r 1 - sum(knn_pred == test$Y) / n_test`. 

#### Le choix de $k$

La valeur de $k$ a été définie de façon arbitraire, on peut optimiser ce choix en essayant différentes valeurs et en choisissant la meilleure. Cela se fait en découpant l'échantillon d'apprentissage en une partie apprentissage et une partie validation. La règle de décision est construite sur la partie apprentissage et les prédictions sont faites sur la partie validation. On peut ainsi mesurer une erreur sur des données qui n'ont pas servi à construire la règle de décision, les données de validation. Pour cela on utilise la fonction `e1071::tune.knn()` qui prend 4 paramètres en entrée : 

* `x` : les prédicteurs
* `y` : le vecteur cible
* `k` : un vecteur de valeurs de $k$ à tester
* `tunecontrol` : la méthode d'échantillonnage, sous la forme d'une fonction `tune.control(sampling = "cross")` pour de la validation croisée (dans ce cas il faut également spécifier l'argument `cross = 5` pour faire de la 5-fold cv) ou `tune.control(sampling = "boot")` pour faire du bootstrap.

```{r knn_tune}
knn_cross_results <- tune.knn(
  x = donnees$X, # predicteurs
  y = donnees$Y, # réponse
  k = 1:50, # essayer knn avec K variant de 1 à 50
  tunecontrol = tune.control(sampling = "cross"), # utilisation de la cross validation
  cross = 5 # 5 blocks
)
```

On peut visualiser les erreurs obtenues sur le graphique suivant.

```{r knn_tune_plot}
ggplot(
  data = knn_cross_results$performances,
  mapping = aes(x = k, y = error)
) + 
  geom_line() +
  geom_point() + 
  labs(
    x = "k",
    y = "Erreur de validation",
    title = "Evolution de l'erreur selon k"
  )
```

Le meilleur $k$ est obtenu avec la commande `knn_cross_results$best.parameters$k` : `r knn_cross_results$best.parameters$k`. De cette manière le choix de $k$ n'est plus arbitraire mais basé sur nos données et le $k$ donnant l'erreur minimale (sur les données d'apprentissage) est choisie. 

_Remarque : la valeur optimale de $k$ peut varier à chaque fois que la commande est lancée. On verra dans l'exercice suivant comment on peut contourner ce problème._

On peut relancer la prédiction avec le $k$ sélectionné et calculer le taux d'erreur comme précédemment. 

```{r knn_opt}
set.seed(123456)
knn_pred <- knn(
  train = as.matrix(donnees$X), # données d'apprentissage
  test = as.matrix(test$X), # données à prédire
  cl = donnees$Y, # vraies valeurs
  k = knn_cross_results$best.parameters$k # nombre de voisins
)
```

L'erreur est de `r 1 - sum(knn_pred == test$Y) / n_test`. 

#### Analyse de l'erreur

L'erreur obtenue est très proche de l'erreur de Bayes calculée à la question 1. Cela est due au fait que les données générées ne correspondent pas parfaitement à la loi définie dans l'énoncé du fait du faible nombre d'observations simulées (200). On peut simuler un nombre supérieur de données, 2000 par exemple, et recalculer l'erreur. Pour ce faire on procède comme précédemment. 

```{r knn_2000}
n_test2 <- 2000
set.seed(3)
X <- rnorm(n_test2)
set.seed(4)
U <- runif(n_test2)
Y2 <- rep(0, n_test2)
Y2[X <= 0 & U <= 0.2] <- 1
Y2[X > 0 & U > 0.4] <- 1
Y <- as.factor(Y2)
test2 <- data.frame(X, Y)
set.seed(123456)
knn_pred2 <- knn(
  train = as.matrix(donnees$X), # données d'apprentissage
  test = as.matrix(test2$X), # données à prédire
  cl = donnees$Y, # vraies valeurs
  k = knn_cross_results$best.parameters$k # nombre de voisins
)
```

L'erreur est maintenant de `r 1 - sum(knn_pred2 == test2$Y) / n_test2`. Elle est bien supérieur à l'erreur minimale théorique. 

## Question 5 - Naive Bayes

>_Quelle est la règle de décision associée au bayésien naïf ? Mettre en oeuvre le Bayésien naïf. Justifier le choix des paramètres et commenter les résultats en validation et sur l'échantillon test._

#### La règle de décision

Avec le modèle bayésien naïf on calcule $\widehat{P}(Y=y|X=x)$ pour $y=0, 1$. Pour choisir la classe $Y$ on va prendre la probabilité la plus forte, c'est à dire celle qui est supérieur à $0,5$.

#### Un premier modèle

La fonction utilisée pour réaliser un modèle bayésien naïf est la fonction `klaR::NaiveBayes()`. Elle prend 2 paramètres en entrée : 

* `formula` : une formule spécifiant le modèle, ici ce sera simplement `Y ~ X`,
* `data` : un data.frame contenant les variables utilisées dans l'argument `formula`

```{r nb}
naive_bayes_model <- NaiveBayes(
  formula = Y ~ X, 
  data = donnees
)
```

On peut utiliser ce modèle pour effectuer des prédictions sur le jeu de données de test et ainsi mesurer l'erreur sur des données non utilisées pour apprendre les probabilités $\widehat{P}(Y=y|X=x)$.

```{r nb_pred}
naive_bayes_pred <- predict(naive_bayes_model, test)
```

Comme précédemment on peut mesurer la précision du modèle ainsi que son erreur de la façon suivante.

```{r nb_pred_perf, eval = FALSE}
# Taux de bonnes prédictions
sum(naive_bayes_pred$class == test$Y) / n_test

# Taux d'erreur
1 - sum(naive_bayes_pred$class == test$Y) / n_test
```

Le taux d'erreur est de `r 1 - sum(naive_bayes_pred$class == test$Y) / n_test`.

#### Le choix des paramètres

Le modèle bayésien naïf a été lancé avec ses paramètres par défaut. 3 hyper-paramètres peuvent être modifiés : 

* l'utilisation ou non d'un noyau pour estimer les densités des variables numériques, `usekernel`
* la largeur de bande utilisée par le noyau (ou plus précisément un facteur multiplicatif qui sera appliqué à une largeur de bande pré-définie), `adjust`
* la correction de Laplace à applique pour les variables catégorielles, `fL`.

Pour choisir la meilleure combinaison d'hyper-paramètres pour le modèle bayésien naïf, on va diviser notre jeu de données en 2 groupes : apprentissage et validation. Comme pour le choix de $k$ dans le modèle des $k$ plus-proches-voisins, le but est d'estimer l'erreur sur les données de validation pour chaque combinaison d'hyper-paramètres et de sélectionner la combinaison avec l'erreur la plus faible. 

On commence par spécifier l'ensemble des combinaisons d'hyper-paramètres que l'on souhaite évaluer. Parmi les 3 hyper-paramètres nécessaires seuls 2 seront utilisés dans notre cas : `usekernel` et `adjust`. En effet la correction de Laplace sert pour les variables catégorielles or notre jeu de données ne comporte que des données numériques. 

```{r nb_param_grid}
hyperparam_grid <- expand.grid(
  usekernel = c(TRUE, FALSE), # si vrai utilisation d'un noyau sinon densité gaussienne
  fL = 0,
  adjust = seq(1, 5, by = 1) # largeur de bande du noyau
)
```

On définit ensuite le type de division apprentissage/validation souhaité. Ici on choisit la validation croisée 5-blocks.

```{r nb_control}
control <- trainControl(method = "cv", number = 5)
```

On peut ensuite entraîner le modèle avec la fonction `caret::train()`. Un modèle par combinaison sera entraîné et l'erreur sera calculée sur l'échantillon de validation.

```{r nb_train}
naive_bayes <- train(
  x = donnees[, "X", drop = FALSE], # prédicteurs
  y = donnees$Y, # réponse
  method = "nb", # classifieur utilisé, ici Naive Bayes
  trControl = control, # méthode d'échantillonnage, ici 5-fold CV
  tuneGrid = hyperparam_grid # combinaisons d'hyper-paramètres à évaluer
)
```

On peut visualiser les résultat sous forme graphique pour choisir la combinaison d'hyper-paramètres donnant le meilleur résultat.

```{r nb_train_plot}
ggplot(
  data = naive_bayes$results,
  mapping = aes(x = adjust, y = Accuracy, color = usekernel)
) + 
  geom_line() + 
  geom_point() + 
  labs(
    x = "Largeur de bande",
    y = "Précision",
    title = "Comparaison des hyper-paramètres",
    color = "Utilisation d'un noyau ?"
  ) + 
  theme(
    legend.position = c(.2, .22)
  )
```

Le meilleur modèle est obtenu avec l'utilisation d'un noyau pour estimer la densité et une largeur de bande de 1 (plus précisément un facteur multiplicatif de la largeur de bande de 1).

On va donc utiliser ces hyper-paramètres pour effectuer une prédiction sur les données de test et ainsi mesurer l'erreur de généralisation du modèle bayésien naïf. La fonction `caret::predict()` va utiliser directement le résultat de la fonction `caret::train()` et donc les meilleurs hyper-paramètres.

```{r nb_predict}
naive_bayes_pred <- predict(
  object = naive_bayes, 
  newdata = test
)
```

Comme précédemment on calcule l'erreur de généralisation. On obtient `r 1 - sum(naive_bayes_pred == test$Y) / n_test`.

```{r nb_error, eval = FALSE}
sum(naive_bayes_pred == test$Y) / n_test
1 - sum(naive_bayes_pred == test$Y) / n_test
```


# Exercice 2 - Discrimination à plusieurs classes

## Question 1 - Import des données

>_Charger la table Vehicle après avoir chargé le package `mlbench`. Lire la description de la table. Combien de prédicteurs potentiels dans cette table ?_

On commence par charger la table `Vehicule` avec la commande `data(Vehicle)` et par avoir un aperçu des données contenues dedans. On peut regarder directement les données en affichant le data.frame, visualiser un résumé avec `summary()` ou encore utiliser `skimr::skim()`.

```{r vehicule, echo = FALSE}
data(Vehicle)
```

La variable cible est `class`, elle comporte 4 modalités. Les autres variables sont toutes quantitatives et peuvent toutes être utilisées comme prédicteurs. 

```{r predicteurs}
predicteurs <- names(Vehicle)[-19]
```

## Question 2 - Fonction de coût

>_Quelle fonction de coût utiliser ?_

Dans le cas d'une classification, la fonction de coût est la précision du modèle. Pour $n$ observations de test on a donc :

$$
\mathscr{L}(y, \widehat{y}) = \sum_{i=1}^{n}1_{y_i\neq\widehat{y}_i}
$$
Pour calculer cette erreur on va séparer les données en un ensemble d'apprentissage et un ensemble de test. On définit une graine pour pouvoir reproduire les résultats et on choisit un ration 80/20. 

```{r split}
set.seed(123456)
train_index <- sample(1:nrow(Vehicle), size = nrow(Vehicle) * .8)
test_index <- setdiff(1:nrow(Vehicle), train_index)
train <- Vehicle[train_index, ]
test <- Vehicle[test_index, ]
```

## Question 3 - Knn

>_Quelle est la règle de décision associée à l'algorithme des k plus proches voisins ? Mettre en oeuvre les k plus proches voisins. Justifir le choix des paramètres et commenter les résultats._

Comme dans le cas précédent on va utiliser le vote à la majorité pour choisir la classe. Le choix de la meilleure valeur de $k$ se fait par validation croisée avec la fonction `e1071::tune.knn()`. On a vu que la valeur retournée pouvait varier. On va ici lancer plusieurs fois cette fonction pour ensuite choisir la valeur de $k$ la plus fréquemment retournée. 

```{r tune_knn_loop}
best_k <- numeric(length = 20)
for (i in 1:20) {
  knn_cross_results <- tune.knn(
    x = scale(train[, predicteurs]),
    y = train[, "Class"],
    k = 1:50,
    tunecontrol = tune.control(sampling = "cross"),
    cross = 5
  )
  best_k[i] <- knn_cross_results$best.parameters[[1]]
  # print(sprintf("Itération %s : meilleur K = %s", i, knn_cross_results$best.parameters))
}
```

```{r tune_knn_loop_table, echo = FALSE}
table <- t(as.data.frame(table(best_k)))
rownames(table) <- c("Valeur de K", "Nombre d'occurences")
kable(
  x = table,
  align = c('c', 'r'), 
  col.names = NA
) %>% 
  kable_styling(full_width = FALSE)
```

On utilise la valeur la plus fréquente pour construire notre modèle de plus-proches-voisins et ainsi mesurer l'erreur sur l'échantillon de test. 

```{r vehicule_knn}
set.seed(123456)
knn_pred <- knn(
  train = as.matrix(scale(train[, predicteurs])), # données d'apprentissage
  test = as.matrix(scale(test[, predicteurs])), # données à prédire
  cl = train[, "Class"], # vraies valeurs
  k = 3 # nombre de voisins
)
```

L'erreur obtenue est `r  1 - sum(knn_pred == test[, "Class"]) / length(test_index)`.

## Question 4 - Naive Bayes

>_Quelle est la règle de décision associée au bayésien naïf ? Mettre en oeuvre le Bayésien naïf. Justifier le choix des paramètres et commenter les résultats._

#### La règle de décision

Comme dans le cas binaire c'est le maximum a posteriori qui est utilisé pour choisir la classe prédite. Les probabilités $P(Y = bus|X)$, $P(Y = saa|X)$, $P(Y = ope|X)$ et $P(Y = van|X)$ sont calculées et la classe choisie correspond à la probabilité la plus élevée. 

#### Le choix des hyper-paramètres

On va ici utiliser la libraire `caret` qui comporte un ensemble de fonction facilitant la création de pipelines d'entraînement en apprentissage supervisé. [Ce livre en ligne](https://topepo.github.io/caret/) et [cette vignette d'introduction](https://cran.r-project.org/web/packages/caret/vignettes/caret.html) donnent une bonne vision du fonctionnement de ce package. On va faire la même chose que dans l'exercice précédent, à savoir évaluer l'erreur de validation pour différentes combinaisons d'hyper-paramètres mais de manière simplifiée. 

On définit l'ensemble des combinaisons à tester. 

```{r vehicule_nb_param_grid}
hyperparam_grid <- expand.grid(
  usekernel = c(TRUE, FALSE), # si vrai utilisation d'un noyau sinon densité gaussienne
  fL = 0,
  adjust = seq(1, 5, by = 1) # largeur de bande du noyau
)
```

On va définir la façon de séparer les données en apprentissage et validation avec la fonction `caret::trainControl()`. On spécifie l'argument `method` à `"cv"` pour faire de la validation croisée et l'argument `number` à `5` pour faire du 5-blocks.

```{r vehicule_nb_control}
control <- trainControl(method = "cv", number = 5)
```

On peut ensuite entraîner le modèle avec la fonction `caret::train()`. Un modèle par combinaison sera entraîné et l'erreur sera calculée sur l'échantillon de validation. La fonction `caret::train()` prend 5 arguments : 

* `x` : les prédicteurs
* `y` : le vecteur de la variable cible
* `method` : le nom du modèle à entraîner, ici `"nb"` pour Naive Bayes
* `trControl` : la méthode de validation
* `tunegrid` : la grille d'hyper-paramètres à tester

```{r vehicule_nb_train}
naive_bayes <- train(
  x = train[, predicteurs], # prédicteurs
  y = train[, "Class"], # réponse
  method = "nb", # classifieur utilisé, ici Naive Bayes
  trControl = control, # méthode d'échantillonnage, ici 5-fold CV
  tuneGrid = hyperparam_grid # combinaisons d'hyper-paramètres à évaluer
)
```

On peut visualiser les résultat sous forme graphique pour choisir la combinaison d'hyper-paramètres donnant le meilleur résultat.

```{r vehicule_nb_train_plot}
ggplot(
  data = naive_bayes$results,
  mapping = aes(x = adjust, y = Accuracy, color = usekernel)
) + 
  geom_line() + 
  geom_point() + 
  labs(
    x = "Largeur de bande",
    y = "Précision",
    title = "Comparaison des hyper-paramètres",
    color = "Utilisation d'un noyau ?"
  ) + 
  theme(
    legend.position = c(.8, .8)
  )
```

Le meilleur modèle est obtenu avec l'utilisation d'un noyau pour estimer la densité et une largeur de bande de 1 (plus précisément un facteur multiplicatif de la largeur de bande de 1).

On va donc utiliser ces hyper-paramètres pour effectuer une prédiction sur les données de test et ainsi mesurer l'erreur de généralisation du modèle bayésien naïf. La fonction `caret::predict()` va utiliser directement le résultat de la fonction `caret::train()` et donc les meilleurs hyper-paramètres.

```{r vehicule_nb_predict}
naive_bayes_pred <- predict(
  object = naive_bayes, 
  newdata = test
)
```


L'erreur de généralisation est de `r round(1 - sum(naive_bayes_pred == test[, "Class"]) / length(test_index), digits = 3)`. 

#### Pourquoi des _warnings_ ?

On peut remarquer que la fonction `predict()` génère des messages de _warning_ `Numerical 0 probability for all classes with observation`. Cela veut dire que les 4 probabilités initialement calculées par le modèle sont toutes très faibles. Leur comparaison n'est donc peut-être pas pertinentes. La fonction renvoie des probabilités normalisées donc ces faibles valeurs n'appariassent pas lorque qu'on regarde les résultats du modèle (utiliser la commande `predict(naive_bayes, test, type = "prob")` pour avoir les probabilités et non pas les classes directement).





<br>

<cite> -- Mathieu Marauri</cite>